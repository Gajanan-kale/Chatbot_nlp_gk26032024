{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4za2SHVCOKO",
    "outputId": "f858a73f-a75d-434b-9afc-aed976133475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9on1hO65CXWt"
   },
   "source": [
    "develope a chatbot using bert\n",
    "ChatGPT\n",
    "Developing a chatbot using BERT (Bidirectional Encoder Representations from Transformers) involves several steps, including fine-tuning a pre-trained BERT model on conversational data and implementing the chatbot interface. Below is a simplified guide to creating a basic chatbot using BERT:\n",
    "\n",
    "1. Preparing the Environment:\n",
    "Install the necessary libraries such as TensorFlow or PyTorch, transformers library (for working with BERT), and any additional dependencies.\n",
    "Download a pre-trained BERT model. You can use Hugging Face's transformers library to easily access pre-trained models.\n",
    "2. Data Collection and Preprocessing:\n",
    "Collect conversational data for fine-tuning the BERT model. This could be chat logs, customer support data, or any other conversational data relevant to your application.\n",
    "Preprocess the data, including tokenization, padding, and formatting it into a suitable format for fine-tuning the BERT model.\n",
    "3. Fine-tuning BERT:\n",
    "Load the pre-trained BERT model using the transformers library.\n",
    "Fine-tune the BERT model on your conversational data using transfer learning techniques. This involves updating the weights of the pre-trained BERT model on your specific task (in this case, conversational response generation).\n",
    "Define appropriate training parameters such as batch size, learning rate, and number of epochs.\n",
    "4. Implementing the Chatbot Interface:\n",
    "Create a simple interface for interacting with the chatbot. This could be a command-line interface, a web-based interface, or integration with messaging platforms like Slack or Telegram.\n",
    "Implement the logic for processing user input and generating responses using the fine-tuned BERT model.\n",
    "Integrate the chatbot with any additional features or services as needed, such as accessing external APIs for information retrieval or performing specific tasks.\n",
    "5. Testing and Evaluation:\n",
    "Test the chatbot with sample inputs to ensure that it responds appropriately to different types of user queries.\n",
    "Evaluate the performance of the chatbot based on criteria such as response relevance, coherence, and engagement.\n",
    "Iterate on the chatbot design and fine-tuning process based on user feedback and performance metrics.\n",
    "Example Python Code (using Hugging Face's Transformers library with PyTorch):\n",
    "python\n",
    "Copy code\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(input_text):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, truncation=True, max_length=128, padding='max_length')\n",
    "\n",
    "    # Convert tokens to tensor\n",
    "    input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor)\n",
    "        predicted_label = torch.argmax(outputs.logits)\n",
    "\n",
    "    # Decode response tokens\n",
    "    response_text = tokenizer.decode(predicted_label, skip_special_tokens=True)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Chatbot loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n",
    "    conversation_history.append(response)\n",
    "\n",
    "    # Exit loop if user indicates end of conversation\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "This is a basic example and may require additional refinement and customization based on your specific use case and requirements. Additionally, fine-tuning BERT on conversational data can be resource-intensive and may require access to powerful hardware or cloud-based computing resources.\n",
    "\n",
    "User\n",
    "using tensorflow\n",
    "ChatGPT\n",
    "Certainly! Using TensorFlow for building a chatbot with BERT involves similar steps as mentioned earlier. Here's how you can implement a chatbot using TensorFlow and the TensorFlow version of BERT:\n",
    "\n",
    "1. Install Dependencies:\n",
    "Make sure you have TensorFlow and the TensorFlow models library installed:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install tensorflow\n",
    "pip install tensorflow-text\n",
    "pip install transformers\n",
    "2. Import Libraries:\n",
    "python\n",
    "Copy code\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "3. Load Pre-trained BERT Model and Tokenizer:\n",
    "python\n",
    "Copy code\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "4. Define Conversation History:\n",
    "python\n",
    "Copy code\n",
    "conversation_history = []\n",
    "5. Define Response Generation Function:\n",
    "python\n",
    "Copy code\n",
    "def generate_response(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, truncation=True, max_length=128, padding='max_length')\n",
    "    input_ids = tf.convert_to_tensor([input_ids])\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predicted_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "\n",
    "    response_text = tokenizer.decode(predicted_label, skip_special_tokens=True)\n",
    "    return response_text\n",
    "6. Chatbot Loop:\n",
    "python\n",
    "Copy code\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n",
    "    conversation_history.append(response)\n",
    "\n",
    "    # Exit loop if user indicates end of conversation\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "Note:\n",
    "This code assumes you have fine-tuned the BERT model for sequence classification on conversational data.\n",
    "Ensure that your TensorFlow and TensorFlow Text versions are compatible with the transformers library you're using.\n",
    "You may need to adapt the code based on the specifics of your fine-tuning process and model architecture.\n",
    "Remember to fine-tune the BERT model on conversational data before deploying the chatbot. Additionally, you can further enhance the chatbot's capabilities by incorporating additional features such as context tracking, sentiment analysis, and entity recognition.\n",
    "\n",
    "User\n",
    "fine tune bert for automotive industry data\n",
    "ChatGPT\n",
    "Fine-tuning BERT for the automotive industry involves adapting a pre-trained BERT model to better understand and generate responses relevant to automotive-related queries or tasks. Here's a high-level overview of the process:\n",
    "\n",
    "1. Data Collection and Preparation:\n",
    "Gather conversational data relevant to the automotive industry. This could include customer support chats, forum discussions, FAQ pages, or any other text data related to automotive products, services, or issues.\n",
    "Preprocess the data, including cleaning, tokenization, and formatting it into a suitable format for fine-tuning BERT.\n",
    "2. Fine-tuning BERT:\n",
    "Load a pre-trained BERT model suitable for your task. Depending on the size of your dataset and the complexity of the task, you can choose from different BERT variants like BERT-base, BERT-large, or domain-specific versions.\n",
    "Fine-tune the BERT model on your automotive industry data using transfer learning techniques. This involves updating the weights of the pre-trained BERT model on your specific task, such as automotive-related question answering or sentiment analysis.\n",
    "Define appropriate training parameters such as batch size, learning rate, and number of epochs. Monitor the training process and adjust parameters as needed to optimize performance.\n",
    "3. Model Evaluation:\n",
    "Evaluate the fine-tuned BERT model on a validation dataset to assess its performance on automotive-related tasks. Use metrics relevant to your specific task, such as accuracy, precision, recall, or F1 score.\n",
    "Fine-tune the model further based on the evaluation results, experimenting with different hyperparameters or training strategies to improve performance.\n",
    "4. Application Development:\n",
    "Integrate the fine-tuned BERT model into your automotive-related applications or services. This could involve building chatbots, virtual assistants, search engines, or recommendation systems tailored to the automotive industry.\n",
    "Develop user interfaces or APIs to interact with the BERT model and provide responses to user queries or inputs.\n",
    "Implement additional features or functionalities as needed, such as entity recognition, sentiment analysis, or context tracking, to enhance the capabilities of your application.\n",
    "5. Testing and Deployment:\n",
    "Test the integrated application thoroughly to ensure that it performs well in real-world scenarios and handles various types of automotive-related queries or tasks effectively.\n",
    "Deploy the application in production environments, making it accessible to users or customers in the automotive industry.\n",
    "Monitor the performance of the deployed application and collect feedback from users to identify areas for improvement and further iteration.\n",
    "Example Use Cases:\n",
    "Customer Support Chatbots: Build chatbots that can provide assistance with common automotive issues, troubleshooting, or product information.\n",
    "Vehicle Recommender Systems: Develop recommendation systems that suggest suitable vehicles based on user preferences, budget, and other criteria.\n",
    "Maintenance and Service Assistants: Create virtual assistants that help users schedule maintenance appointments, find service centers, or diagnose vehicle problems.\n",
    "Voice-Activated Interfaces: Implement voice-controlled interfaces for in-car systems that allow drivers to access navigation, entertainment, or communication features hands-free.\n",
    "By following these steps and customizing the fine-tuning process to the specific needs of the automotive industry, you can leverage the power of BERT to build intelligent applications that enhance the automotive experience for users and customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzjuI50ECr0k"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "pip install tensorflow-text\n",
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cdQVN6nDYjp",
    "outputId": "cc3e68bd-815b-470d-84d2-47ac5093eb01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.25.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.0.7)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz1DK3iLEPSR",
    "outputId": "923d7a26-e213-49ce-c648-f9d7d14cf4c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "nAZv1rRMC2sl",
    "outputId": "ad6b9b02-dd14-4dc0-84cb-4175d0ddd57f"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nmodule 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mactivations_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tf_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from ...modeling_tf_outputs import (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/activations_tf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/__internal__/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_initialize_variables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minitialize_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrack_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtLarge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/applications/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/sequential.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlegacy_sm_saving_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msaved_model_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/saving/legacy/saved_model/load_context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_load_context_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_load_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-74f58e7871f2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1463\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1475\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nmodule 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AQiI5B3kE0gf",
    "outputId": "e70b4d2d-0caf-4ba9-d333-2ae6b54161a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "Successfully installed transformers-4.39.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "d5d5607e88994f2fb8a0a5a9092c6f8f",
       "pip_warning": {
        "packages": [
         "transformers"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "FyNqk0CoC5B3",
    "outputId": "7850c397-9937-47e3-a3f6-ed410228c581"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b65ee97c6c67>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojwX3x69C_aY"
   },
   "outputs": [],
   "source": [
    "conversation_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWB9HeOFDBCv"
   },
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, truncation=True, max_length=128, padding='max_length')\n",
    "    input_ids = tf.convert_to_tensor([input_ids])\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predicted_label = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "\n",
    "    response_text = tokenizer.decode(predicted_label, skip_special_tokens=True)\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pf4nqA1DDew"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n",
    "    conversation_history.append(response)\n",
    "\n",
    "    # Exit loop if user indicates end of conversation\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg-VrP6YDNl9"
   },
   "source": [
    "This code assumes you have fine-tuned the BERT model for sequence classification on conversational data.\n",
    "Ensure that your TensorFlow and TensorFlow Text versions are compatible with the transformers library you're using.\n",
    "You may need to adapt the code based on the specifics of your fine-tuning process and model architecture.\n",
    "Remember to fine-tune the BERT model on conversational data before deploying the chatbot. Additionally, you can further enhance the chatbot's capabilities by incorporating additional features such as context tracking, sentiment analysis, and entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6qHH8v1F7lZ"
   },
   "source": [
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a deep learning model introduced by Google in 2018 for natural language processing (NLP) tasks.\n",
    "\n",
    "It belongs to the Transformer architecture family, which has revolutionized NLP due to its parallelization capabilities and ability to capture long-range dependencies in text.\n",
    "\n",
    "BERT, in particular, is designed to understand the context of words in a sentence by considering the words that come before and after each word.\n",
    "\n",
    "Here's a detailed explanation of BERT along with an example:\n",
    "\n",
    "Architecture:\n",
    "\n",
    "BERT is built on the Transformer architecture, which includes self-attention mechanisms and feed-forward neural networks.\n",
    "\n",
    "It consists of multiple layers of encoders. Each encoder layer utilizes multi-head self-attention mechanisms and position-wise fully connected feed-forward networks.\n",
    "\n",
    "BERT uses a bidirectional approach, meaning it considers both left and right contexts of a word to understand its meaning within the sentence.\n",
    "\n",
    "Pre-training:\n",
    "\n",
    "BERT is pre-trained on large text corpora using two unsupervised tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP).\n",
    "\n",
    "MLM randomly masks some of the words in the input sentence and trains the model to predict the masked words based on the surrounding context.\n",
    "\n",
    "NSP involves predicting whether a sentence follows another sentence in the given text.\n",
    "\n",
    "Through pre-training on these tasks, BERT learns rich contextual representations of words.\n",
    "\n",
    "Fine-tuning:\n",
    "\n",
    "After pre-training, BERT can be fine-tuned on specific downstream tasks such as text classification, named entity recognition, question answering, etc.\n",
    "\n",
    "During fine-tuning, task-specific layers are added on top of the pre-trained BERT model, and the entire model is fine-tuned on the labeled task-specific data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider the sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "BERT tokenizes this sentence into individual tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"].\n",
    "\n",
    "BERT adds special tokens [CLS] at the beginning and [SEP] between sentences if there are multiple sentences.\n",
    "\n",
    "The tokenized sentence may look like: [\"[CLS]\", \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"[SEP]\"].\n",
    "\n",
    "BERT then converts these tokens into embeddings, which capture the contextual information of each token considering its surrounding tokens.\n",
    "\n",
    "During pre-training, some tokens are randomly masked.\n",
    "\n",
    "For example, \"cat\" might be masked as \"[MASK]\".\n",
    "\n",
    "BERT is trained to predict the masked word based on the context provided by the surrounding tokens.\n",
    "\n",
    "After pre-training, for a downstream task like text classification, BERT's pre-trained layers are fine-tuned on task-specific labeled data to classify text into categories.\n",
    "\n",
    "BERT's ability to capture contextual information bidirectionally has significantly improved the performance of various NLP tasks, making it one of the most influential models in the field of natural language processing.\n",
    "\n",
    "Use of bert in sentiment analysis:\n",
    "\n",
    "BERT can be effectively used in sentiment analysis tasks due to its ability to capture rich contextual information from text data. Here's how BERT can be applied in sentiment analysis:\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "Tokenize the input text into individual tokens.\n",
    "\n",
    "Add special tokens ([CLS] for classification, [SEP] to separate sentences if needed).\n",
    "\n",
    "Convert tokens into corresponding embeddings using BERT's pre-trained word embeddings.\n",
    "\n",
    "Fine-tuning:\n",
    "\n",
    "Initialize a classification layer on top of the pre-trained BERT model.\n",
    "\n",
    "Fine-tune the entire model, including BERT's pre-trained layers and the added classification layer, on sentiment analysis datasets.\n",
    "\n",
    "During fine-tuning, the model learns to map the contextual embeddings from BERT to sentiment labels (e.g., positive, negative, neutral).\n",
    "\n",
    "Inference:\n",
    "\n",
    "After fine-tuning, the BERT-based sentiment analysis model can be used to predict the sentiment of new text inputs.\n",
    "\n",
    "Pass the tokenized and embedded input text through the fine-tuned BERT model.\n",
    "\n",
    "Obtain the output from the classification layer, which represents the predicted sentiment label for the input text.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say we have a sentence: \"I absolutely loved the movie! It was fantastic.\"\n",
    "\n",
    "Tokenize the sentence: [\"[CLS]\", \"I\", \"absolutely\", \"loved\", \"the\", \"movie\", \"!\", \"It\", \"was\", \"fantastic\", \".\", \"[SEP]\"].\n",
    "\n",
    "Convert tokens into embeddings using BERT's pre-trained word embeddings.\n",
    "\n",
    "Pass the embeddings through the fine-tuned BERT model.\n",
    "\n",
    "Obtain the output from the classification layer, which predicts the sentiment label for the input text (e.g., positive).\n",
    "\n",
    "Using BERT for sentiment analysis offers several advantages:\n",
    "\n",
    "BERT captures the contextual meaning of words, allowing it to understand nuances and sentiment cues in text.\n",
    "\n",
    "Fine-tuning BERT on sentiment analysis tasks requires less labeled data compared to training a sentiment analysis model from scratch, as BERT has already learned rich contextual representations during pre-training.\n",
    "\n",
    "BERT's bidirectional approach ensures that it considers both preceding and succeeding words in a sentence, improving sentiment analysis accuracy.\n",
    "\n",
    "Overall, leveraging BERT for sentiment analysis enables more accurate and context-aware sentiment predictions, making it a powerful tool for analyzing sentiment in text data.\n",
    "\n",
    "Large Language Models :\n",
    "\n",
    "(LLMs) are a class of artificial intelligence models designed to understand, generate, and process human language.\n",
    "\n",
    "These models have gained significant attention and advancement in recent years due to their ability to perform a wide range of natural language processing (NLP) tasks with remarkable accuracy.\n",
    "\n",
    "LLMs are typically built using deep learning architectures, particularly variants of recurrent neural networks (RNNs) or transformers.\n",
    "\n",
    "Here's an explanation of LLMs and their types:\n",
    "\n",
    "Large Language Models (LLMs):\n",
    "Definition: LLMs are neural network-based models trained on massive amounts of text data. They are capable of understanding and generating human-like text, answering questions, completing sentences, translating languages, and performing various other NLP tasks.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "Scale: LLMs are trained on vast datasets, often comprising billions or even trillions of words, allowing them to capture complex linguistic patterns and semantics.\n",
    "\n",
    "Representation: They learn distributed representations of words and sentences, capturing semantic relationships and contextual information.\n",
    "\n",
    "Parameter Size: LLMs have a large number of parameters, ranging from millions to billions, enabling them to capture fine-grained details of language.\n",
    "\n",
    "Pre-training and Fine-tuning: They are typically pre-trained on large corpora using unsupervised or semi-supervised learning methods, followed by fine-tuning on task-specific datasets to adapt them to specific downstream tasks.\n",
    "\n",
    "Transformer Architecture:\n",
    "\n",
    "Many state-of-the-art LLMs are based on transformer architectures, which have demonstrated superior performance in handling sequential data like text.\n",
    "\n",
    "2.Types of Large Language Models:\n",
    "\n",
    "a. Pre-trained LLMs:\n",
    "\n",
    "These models are trained on massive text corpora using self-supervised learning objectives such as masked language modeling (MLM) or next sentence prediction (NSP).\n",
    "\n",
    "Examples include BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and RoBERTa (Robustly Optimized BERT Approach).\n",
    "\n",
    "b. Generative LLMs:\n",
    "\n",
    "These models are capable of generating coherent and contextually relevant text, often used for tasks like text completion, dialogue generation, and text summarization.\n",
    "\n",
    "Examples include GPT (Generative Pre-trained Transformer) and GPT-3 (Generative Pre-trained Transformer 3).\n",
    "\n",
    "c. Encoder-Decoder LLMs:\n",
    "\n",
    "These models consist of separate encoder and decoder components and are commonly used for sequence-to-sequence tasks like machine translation, text summarization, and conversational agents.\n",
    "\n",
    "Examples include BERT-to-BERT and T5 (Text-To-Text Transfer Transformer).\n",
    "\n",
    "d. Multimodal LLMs:\n",
    "\n",
    "These models integrate both text and other modalities such as images, audio, or video, enabling them to understand and generate multimodal content.\n",
    "\n",
    "Examples include CLIP (Contrastive Language-Image Pre-training) and ViT (Vision Transformer).\n",
    "\n",
    "e. Task-specific LLMs:\n",
    "\n",
    "These models are fine-tuned on specific downstream tasks, such as sentiment analysis, named entity recognition (NER), or machine translation, to achieve optimal performance for those tasks.\n",
    "\n",
    "Examples include BERT for sentiment analysis, RoBERTa for NER, and GPT-2 for text generation.\n",
    "\n",
    "In summary, Large Language Models are a versatile class of AI models that have revolutionized natural language understanding and generation tasks. They come in various types, each tailored to specific NLP applications and exhibiting different capabilities and characteristics.\n",
    "\n",
    "Prompt engineering is a technique used in natural language processing (NLP) to guide the behavior of large language models (LLMs) such as GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers) towards generating desired outputs or responses. It involves crafting specific prompts or input instructions that steer the model towards generating text that aligns with a particular task or objective.\n",
    "\n",
    "Here's a detailed explanation of prompt engineering:\n",
    "\n",
    "1. Understanding the Objective:\n",
    "\n",
    "Prompt engineering starts with a clear understanding of the task or objective you want the language model to perform. This could be text completion, question answering, summarization, translation, sentiment analysis, or any other NLP task.\n",
    "\n",
    "2. Crafting the Prompt:\n",
    "\n",
    "Prompt engineering involves crafting a well-designed prompt that conveys the task or context to the language model effectively. The prompt provides the initial input to the model, influencing its subsequent generation.\n",
    "\n",
    "3. Language Modeling with Prompts:\n",
    "\n",
    "When presented with a prompt, the language model generates text conditioned on the provided input. The generated text is influenced by both the prompt and the internal representations learned during pre-training.\n",
    "\n",
    "4. Iterative Refinement:\n",
    "\n",
    "Prompt engineering often involves an iterative process of experimentation and refinement. Engineers and researchers iteratively adjust and fine-tune the prompts based on the model's outputs, aiming to improve performance and alignment with the task objective.\n",
    "\n",
    "5. Leveraging Model Capabilities:\n",
    "\n",
    "Effective prompt engineering leverages the capabilities of the underlying language model architecture. For example, in GPT-like models, prompts can include specific tokens such as \"Generate a headline:\", \"Translate the following sentence to French:\", or \"Complete the sentence:\". These tokens guide the model's generation process towards the desired task.\n",
    "\n",
    "6. Domain-Specific Knowledge:\n",
    "\n",
    "Domain-specific knowledge plays a crucial role in prompt engineering. Engineers and domain experts design prompts that incorporate domain-specific terminology, constraints, or requirements to ensure the model's outputs are contextually relevant and accurate.\n",
    "\n",
    "7. Evaluation and Validation:\n",
    "\n",
    "Prompt engineering involves evaluating the effectiveness of the crafted prompts through empirical validation. This may include human evaluation, automated metrics, or downstream task performance assessment to measure the quality and effectiveness of the model's outputs.\n",
    "\n",
    "8. Fine-Tuning and Adaptation:\n",
    "\n",
    "Prompt engineering can be further enhanced through fine-tuning or adaptation of the language model to specific tasks or domains. Fine-tuning involves training the model on task-specific datasets, adjusting the model's parameters to better align with the task requirements.\n",
    "\n",
    "9. Ethical Considerations:\n",
    "\n",
    "Prompt engineering raises ethical considerations regarding bias, fairness, and unintended consequences in the model's outputs. Engineers must carefully design prompts to mitigate bias and ensure responsible AI practices.\n",
    "\n",
    "10. Continuous Improvement:\n",
    "\n",
    "Prompt engineering is an ongoing process that evolves with advancements in NLP research and applications. Continuous experimentation, innovation, and adaptation are key to improving prompt design and maximizing the effectiveness of language models in various tasks.\n",
    "\n",
    "In summary, prompt engineering is a critical aspect of leveraging large language models for NLP tasks. By carefully crafting prompts that guide the model's behavior towards desired outcomes, engineers and researchers can harness the power of language models to solve a wide range of real-world challenges.\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) system is a type of natural language processing (NLP) model that combines elements of retrieval-based and generative models to improve the quality and relevance of generated text.\n",
    "\n",
    "In a RAG system, a generative model such as GPT (Generative Pre-trained Transformer) is augmented with a retrieval-based component, typically in the form of a dense retrieval model or a traditional search engine.\n",
    "\n",
    "How a Retrieval-Augmented Generation system works:\n",
    "\n",
    "1. Retrieval Component:\n",
    "\n",
    "The retrieval component of the RAG system is responsible for retrieving relevant information or context from a large corpus of text. This could be accomplished using dense retrieval models like DPR (Dense Passage Retrieval) or traditional search engines like Elasticsearch.\n",
    "\n",
    "The retrieval component uses the input prompt or query to search for relevant passages or documents from the corpus. These passages serve as context or evidence for the generative model to generate coherent and contextually relevant responses.\n",
    "\n",
    "2. Generation Component:\n",
    "\n",
    "The generation component of the RAG system is typically a large language model such as GPT. It takes the retrieved passages as input along with the initial prompt or query and generates text based on this combined information.\n",
    "\n",
    "The generative model leverages the retrieved context to produce responses that are more informed, coherent, and contextually relevant compared to traditional generative models operating in isolation.\n",
    "\n",
    "3. Integration of Retrieval and Generation:\n",
    "\n",
    "The retrieved passages are integrated into the input sequence fed to the generative model. This integration can be achieved by concatenating the retrieved passages with the input prompt or by providing them as additional context through attention mechanisms. The generative model processes the combined input, attending to both the prompt and the retrieved passages to produce the final output.\n",
    "\n",
    "4. Fine-Tuning and Training:\n",
    "\n",
    "RAG systems are typically fine-tuned on task-specific datasets to adapt them to specific applications or domains. Fine-tuning involves training the generative model along with the retrieval component on a task-specific objective, such as question answering, summarization, or dialogue generation. During training, the model learns to effectively utilize the retrieved context to generate high-quality responses that are relevant to the input prompt or query.\n",
    "\n",
    "5. Benefits of RAG Systems:\n",
    "\n",
    "Enhanced Relevance: By incorporating retrieved context, RAG systems produce responses that are more contextually relevant and informative compared to traditional generative models.\n",
    "\n",
    "Coherence and Consistency: The retrieved passages provide additional context that helps the generative model produce more coherent and consistent responses. Flexibility: RAG systems are flexible and can be adapted to a wide range of NLP tasks, including question answering, dialogue generation, content creation, and more.\n",
    "\n",
    "6. Applications of RAG Systems:\n",
    "\n",
    "Question Answering: RAG systems can be used to answer questions by retrieving relevant passages from a knowledge base and generating concise answers based on the retrieved context.\n",
    "\n",
    "Conversational AI: RAG systems enhance the capabilities of chatbots and virtual assistants by providing them with access to a large corpus of knowledge and contextually relevant information.\n",
    "\n",
    "Content Generation: RAG systems can generate coherent and informative content for various applications such as summarization, content recommendation, and content creation.\n",
    "\n",
    "In summary, Retrieval-Augmented Generation (RAG) systems leverage the strengths of both retrieval-based and generative models to produce high-quality, contextually relevant text generation. By integrating retrieved context into the generative process, RAG systems achieve superior performance across a wide range of NLP tasks and applications.\n",
    "\n",
    "Using a Retrieval-Augmented Generation (RAG) system in a chatbot enhances its ability to generate contextually relevant and informative responses by leveraging both retrieval-based and generative models. Below, I'll outline how to implement a simple chatbot using a RAG system with an example code snippet:\n",
    "\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration import torch\n",
    "\n",
    "Initialize RAG tokenizer and retriever\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\") retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "\n",
    "Initialize RAG sequence model for generation\n",
    "\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "\n",
    "Define function for chatbot interaction\n",
    "\n",
    "def chatbot(prompt): input_ids = tokenizer([prompt], return_tensors=\"pt\")[\"input_ids\"] retriever_results = model.retrieve(input_ids) generator_input = {\"input_ids\": input_ids, \"retrieved_doc_ids\": retriever_results[\"doc_ids\"]} generated = model.generate(**generator_input, max_length=50, num_return_sequences=1) generated_text = tokenizer.batch_decode(generated, skip_special_tokens=True)[0] return generated_text\n",
    "\n",
    "Example usage\n",
    "\n",
    "prompt = \"Tell me about artificial intelligence.\" response = chatbot(prompt) print(\"Chatbot Response:\", response)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Initializing RAG components: First, we initialize the RAG tokenizer, retriever, and sequence model for generation using the Facebook RAG model. The tokenizer is responsible for tokenizing input text, the retriever retrieves relevant passages from a corpus, and the sequence model generates responses.\n",
    "\n",
    "Chatbot function:\n",
    "\n",
    "We define a function chatbot(prompt) that takes a user prompt as input. Inside the function:\n",
    "\n",
    "We tokenize the input prompt and retrieve relevant documents using the retriever component. We generate a response using the sequence model, providing both the input prompt and retrieved document IDs as input.\n",
    "\n",
    "The generated response is decoded from token IDs to text and returned as the chatbot's response. Example usage: We provide an example prompt to the chatbot and print the generated response.\n",
    "\n",
    "This example demonstrates a basic chatbot implementation using a RAG system. In practice, you would fine-tune the RAG model on task-specific data and further customize the chatbot's behavior based on your requirements. Additionally, you can integrate the chatbot with other components such as user interfaces or messaging platforms for real-world deployment.\n",
    "\n",
    "[ ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4t7fgz6GGK6r",
    "outputId": "5b1b2399-019d-41c7-d33e-1887d669cd04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "Intent Counts: {'car models - coupe info': 98, 'car models - hatchback info': 98, 'car models - truck info': 98, 'car brands': 1, 'car features': 1, 'car models - sedan info': 1, 'car models - SUV info': 98, 'car maintenance': 1, 'greetings': 1, 'car models': 1}\n",
      "Intent Counts: {'car models - coupe info': 98, 'car models - hatchback info': 98, 'car models - truck info': 98, 'car brands': 1, 'car features': 1, 'car models - sedan info': 99, 'car models - SUV info': 98, 'car maintenance': 1, 'greetings': 1, 'car models': 1}\n",
      "data: 496\n",
      "496\n",
      "Intent Counts after balancing: {'car models - hatchback info': 98, 'car models - truck info': 98, 'car brands': 100, 'car features': 100, 'car models - sedan info': 99, 'car models - SUV info': 98, 'car maintenance': 100, 'car models - coupe info': 98, 'car models': 100, 'greetings': 100}\n",
      "hilen: What SUV options do you have available?\n",
      "991\n",
      "991\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# Define response templates for each intent\n",
    "response_templates = {\n",
    "        'car models': \"Our available models include sedan, SUV, coupe, hatchback, and truck.\",\n",
    "        'car brands': \"We offer vehicles from top brands such as Toyota, Honda, Ford, Chevrolet, and Tesla.\",\n",
    "        'car features': \"Our vehicles come with a range of features including engine options, transmission types, fuel efficiency, and safety features.\",\n",
    "        'car maintenance': \"Our maintenance services include oil change, tire rotation, brake inspection, battery check, and fluid level check.\",\n",
    "        'greetings': \"Hello! How can I assist you today?\"\n",
    "    }\n",
    "\n",
    "# Add response for each specific car model\n",
    "response_templates['car models - sedan info'] = \"Our sedan models offer a perfect combination of style, comfort, and performance. Explore our range of sedan models for the latest in technology, safety, and luxury features.\"\n",
    "response_templates['car models - SUV info'] = \"Our SUV lineup includes versatile models designed for all your adventures. Discover spacious interiors, advanced safety features, and powerful performance in our SUVs.\"\n",
    "response_templates['car models - coupe info'] = \"Experience the thrill of driving with our coupe models. With sleek designs and powerful engines, our coupes deliver an exhilarating performance.\"\n",
    "response_templates['car models - hatchback info'] = \"Our hatchback models offer practicality and versatility in a compact package. Enjoy agile handling, ample cargo space, and fuel-efficient engines in our hatchbacks.\"\n",
    "response_templates['car models - truck info'] = \"Get the job done with our rugged and reliable truck models. From hauling heavy loads to off-road adventures, our trucks are built to tackle any task with ease.\"\n",
    "\n",
    "# Define intents and example queries\n",
    "intents = {\n",
    "    'data': [\n",
    "        'car models sedan SUV coupe hatchback truck',\n",
    "        'car brands Toyota Honda Ford Chevrolet Tesla',\n",
    "        'car features engine transmission fuel efficiency safety features interior features',\n",
    "        'car maintenance oil change tire rotation brake inspection battery check fluid level check',\n",
    "        'greetings hi hello hey hola howdy',\n",
    "        \"I want to know about sedan models.\",\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ],\n",
    "    'target': ['car models', 'car brands', 'car features', 'car maintenance', 'greetings','car models - sedan info']\n",
    "}\n",
    "rephrased_queries = [\n",
    "    \"Can you provide information on sedan options?\",\n",
    "    \"What sedan models do you offer?\",\n",
    "    \"I'm curious about your sedan lineup.\",\n",
    "    \"Tell me more about the sedans you have available.\",\n",
    "    \"Could you give me details about your sedan selection?\",\n",
    "    \"I'm interested in knowing about your sedan inventory.\",\n",
    "    \"Can you share details about the sedans in your showroom?\",\n",
    "    \"I want to learn more about sedans.\",\n",
    "    \"Do you have any information on sedans?\",\n",
    "    \"Could you provide an overview of your sedan models?\",\n",
    "    \"I'm looking for details on sedan vehicles.\",\n",
    "    \"What can you tell me about sedans?\",\n",
    "    \"I'm seeking information about sedan options.\",\n",
    "    \"Can you tell me about your sedan lineup?\",\n",
    "    \"I'd like to know more about sedans.\",\n",
    "    \"Tell me about the sedan models you offer.\",\n",
    "    \"What sedan options do you have?\",\n",
    "    \"I'm interested in sedan cars.\",\n",
    "    \"Can you provide an overview of sedans?\",\n",
    "    \"What sedan models are currently available?\",\n",
    "    \"I want to know about the sedans you sell.\",\n",
    "    \"Could you give me some information on sedans?\",\n",
    "    \"Tell me about your sedan inventory.\",\n",
    "    \"Can you provide details about sedan choices?\",\n",
    "    \"What sedans do you have in stock?\",\n",
    "    \"I'd like to learn about sedan options.\",\n",
    "    \"Can you tell me about sedan vehicles?\",\n",
    "    \"What sedan models do you carry?\",\n",
    "    \"Could you give me details on sedan options?\",\n",
    "    \"I'm curious about sedan choices.\",\n",
    "    \"Can you provide more information on sedan models?\",\n",
    "    \"What sedans are available for purchase?\",\n",
    "    \"I want to know more about sedan vehicles.\",\n",
    "    \"Tell me about the sedans you offer.\",\n",
    "    \"What are my options for sedan cars?\",\n",
    "    \"I'm interested in sedan options.\",\n",
    "    \"Can you give me details on sedan choices?\",\n",
    "    \"Tell me about the sedans you have in-store.\",\n",
    "    \"What sedan models can I choose from?\",\n",
    "    \"I want to explore sedan models.\",\n",
    "    \"Can you provide specifics on sedan options?\",\n",
    "    \"What sedans are currently in stock?\",\n",
    "    \"I'm seeking information on sedan vehicles.\",\n",
    "    \"Tell me about your selection of sedans.\",\n",
    "    \"What sedan models do you have available?\",\n",
    "    \"I'm interested in learning about sedans.\",\n",
    "    \"Can you tell me about the sedan cars you sell?\",\n",
    "    \"What sedan models are offered at your dealership?\",\n",
    "    \"I want to know about sedan options.\",\n",
    "    \"Can you provide details on sedan models?\",\n",
    "    \"Tell me about your sedan lineup.\",\n",
    "    \"What sedans do you currently have?\",\n",
    "    \"I want to learn more about sedan cars.\",\n",
    "    \"Can you give me information on sedan choices?\",\n",
    "    \"What sedan vehicles are in your showroom?\",\n",
    "    \"I'm interested in knowing about sedan models.\",\n",
    "    \"Tell me about the sedan models you offer.\",\n",
    "    \"What sedan options do you have available?\",\n",
    "    \"I'd like to know about sedan choices.\",\n",
    "    \"Can you provide an overview of sedan vehicles?\",\n",
    "    \"I want to explore sedan options.\",\n",
    "    \"Can you tell me about the sedan models you carry?\",\n",
    "    \"I'm curious about sedan models.\",\n",
    "    \"What sedan cars are currently in stock?\",\n",
    "    \"I want to know more about sedan choices.\",\n",
    "    \"Can you provide details on your sedan inventory?\",\n",
    "    \"Tell me about the sedan models at your dealership.\",\n",
    "    \"What sedans do you have on display?\",\n",
    "    \"I'm interested in sedan models.\",\n",
    "    \"Can you give me information on your sedan selection?\",\n",
    "    \"What sedan models are part of your inventory?\",\n",
    "    \"I want to know about the sedans you offer.\",\n",
    "    \"Tell me about your sedan car lineup.\",\n",
    "    \"What sedan vehicles do you have available?\",\n",
    "    \"I'm curious about your sedan car selection.\",\n",
    "    \"Can you provide an overview of sedan cars?\",\n",
    "    \"What sedan models can I expect to find?\",\n",
    "    \"I want to learn about sedan car options.\",\n",
    "    \"Can you give me details on your sedan lineup?\",\n",
    "    \"Tell me about the sedans you have in-store.\",\n",
    "    \"I'm seeking information on sedan cars.\",\n",
    "    \"Can you provide details on sedan car choices?\",\n",
    "    \"What sedan models are currently in your inventory?\",\n",
    "    \"I want to know more about sedan car choices.\",\n",
    "    \"Can you tell me about sedan cars?\",\n",
    "    \"I'm interested in your selection of sedan vehicles.\",\n",
    "    \"Tell me about the sedan models available.\",\n",
    "    \"What sedan options are available for purchase?\",\n",
    "    \"I want to explore sedan car options.\",\n",
    "    \"Can you provide specifics on sedan models?\",\n",
    "    \"What sedans do you have in stock right now?\",\n",
    "    \"I'm seeking information on sedan car models.\",\n",
    "    \"Tell me about your selection of sedan cars.\",\n",
    "    \"What sedan models do you have available for sale?\",\n",
    "    \"I'd like to know about the sedans you sell.\",\n",
    "    \"Can you give me details on sedan car options?\",\n",
    "    \"What sedan vehicles are currently in your showroom?\",\n",
    "    \"I want to learn more about sedan cars\"\n",
    "]\n",
    "\n",
    "# Define categories\n",
    "categories = ['SUV', 'coupe', 'hatchback', 'truck']\n",
    "\n",
    "# Common rephrased queries template\n",
    "template = [\n",
    "    \"Can you provide information on {} options?\",\n",
    "    \"What {} models do you offer?\",\n",
    "    \"I'm curious about your {} lineup.\",\n",
    "    \"Tell me more about the {}s you have available.\",\n",
    "    \"Could you give me details about your {} selection?\",\n",
    "    \"I'm interested in knowing about your {} inventory.\",\n",
    "    \"Can you share details about the {}s in your showroom?\",\n",
    "    \"I want to learn more about {}s.\",\n",
    "    \"Do you have any information on {}s?\",\n",
    "    \"Could you provide an overview of your {} models?\",\n",
    "    \"I'm looking for details on {} vehicles.\",\n",
    "    \"What can you tell me about {}s?\",\n",
    "    \"I'm seeking information about {} options.\",\n",
    "    \"Can you tell me about your {} lineup?\",\n",
    "    \"I'd like to know more about {}s.\",\n",
    "    \"Tell me about the {} models you offer.\",\n",
    "    \"What {} options do you have?\",\n",
    "    \"I'm interested in {}s.\",\n",
    "    \"Can you provide an overview of {}s?\",\n",
    "    \"What {} models are currently available?\",\n",
    "    \"I want to know about the {}s you sell.\",\n",
    "    \"Could you give me some information on {}s?\",\n",
    "    \"Tell me about your {} inventory.\",\n",
    "    \"Can you provide details about {} choices?\",\n",
    "    \"What {}s do you have in stock?\",\n",
    "    \"I'd like to learn about {} options.\",\n",
    "    \"Can you tell me about {} vehicles?\",\n",
    "    \"What {} models do you carry?\",\n",
    "    \"Could you give me details on {} options?\",\n",
    "    \"I'm curious about {} choices.\",\n",
    "    \"Can you provide more information on {} models?\",\n",
    "    \"What {}s are available for purchase?\",\n",
    "    \"I want to know more about {} vehicles.\",\n",
    "    \"Tell me about the {}s you offer.\",\n",
    "    \"What are my options for {} cars?\",\n",
    "    \"I'm interested in {} options.\",\n",
    "    \"Can you give me details on {} choices?\",\n",
    "    \"Tell me about the {}s you have in-store.\",\n",
    "    \"What {} models can I choose from?\",\n",
    "    \"I want to explore {} models.\",\n",
    "    \"Can you provide specifics on {} options?\",\n",
    "    \"What {}s are currently in stock?\",\n",
    "    \"I'm seeking information on {} vehicles.\",\n",
    "    \"Tell me about your selection of {}s.\",\n",
    "    \"What {} models do you have available?\",\n",
    "    \"I'm interested in learning about {}s.\",\n",
    "    \"Can you tell me about the {} cars you sell?\",\n",
    "    \"What {} models are offered at your dealership?\",\n",
    "    \"I want to know about {} options.\",\n",
    "    \"Can you provide details on {} models?\",\n",
    "    \"Tell me about your {} lineup.\",\n",
    "    \"What {}s do you currently have?\",\n",
    "    \"I want to learn more about {} cars.\",\n",
    "    \"Can you give me information on {} choices?\",\n",
    "    \"What {} vehicles are in your showroom?\",\n",
    "    \"I'm interested in knowing about {} models.\",\n",
    "    \"Tell me about the {} models you offer.\",\n",
    "    \"What {} options do you have available?\",\n",
    "    \"I'd like to know about {} choices.\",\n",
    "    \"Can you provide an overview of {} vehicles?\",\n",
    "    \"I want to explore {} options.\",\n",
    "    \"Can you tell me about the {} models you carry?\",\n",
    "    \"I'm curious about {} models.\",\n",
    "    \"What {} cars are currently in stock?\",\n",
    "    \"I want to know more about {} choices.\",\n",
    "    \"Can you provide details on your {} inventory?\",\n",
    "    \"Tell me about the {} models at your dealership.\",\n",
    "    \"What {}s do you have on display?\",\n",
    "    \"I'm interested in {} models.\",\n",
    "    \"Can you give me information on your {} selection?\",\n",
    "    \"What {} models are part of your inventory?\",\n",
    "    \"I want to know about the {}s you offer.\",\n",
    "    \"Tell me about your {} car lineup.\",\n",
    "    \"What {} vehicles do you have available?\",\n",
    "    \"I'm curious about your {} car selection.\",\n",
    "    \"Can you provide an overview of {} cars?\",\n",
    "    \"What {} models can I expect to find?\",\n",
    "    \"I want to learn about {} car options.\",\n",
    "    \"Can you give me details on your {} lineup?\",\n",
    "    \"Tell me about the {}s you have in-store.\",\n",
    "    \"I'm seeking information on {} cars.\",\n",
    "    \"Can you provide details on {} car choices?\",\n",
    "    \"What {} models are currently in your inventory?\",\n",
    "    \"I want to know more about {} car choices.\",\n",
    "    \"Can you tell me about {} cars?\",\n",
    "    \"I'm interested in your selection of {} vehicles.\",\n",
    "    \"Tell me about the {} models available.\",\n",
    "    \"What {} options are available for purchase?\",\n",
    "    \"I want to explore {} car options.\",\n",
    "    \"Can you provide specifics on {} models?\",\n",
    "    \"What {}s do you have in stock right now?\",\n",
    "    \"I'm seeking information on {} car models.\",\n",
    "    \"Tell me about your selection of {} cars.\",\n",
    "    \"What {} models do you have available for sale?\",\n",
    "    \"I'd like to know about the {}s you sell.\",\n",
    "    \"Can you give me details on {} car options?\",\n",
    "    \"What {} vehicles are currently in your showroom?\",\n",
    "    \"I want to learn more about {} cars\"\n",
    "]\n",
    "print(len(template))\n",
    "# Generate rephrased queries for each category\n",
    "for category in categories:\n",
    "    rephrased_queries1 = [query.format(category) for query in template]\n",
    "    intents['data'].extend(rephrased_queries1)\n",
    "    intents['target'].extend([f'car models - {category} info' for _ in range(len(rephrased_queries1))])\n",
    "\n",
    "    #print(f\"{category.capitalize()}:\")\n",
    "    #for query in rephrased_queries:\n",
    "    #print(query)\n",
    "    #print()\n",
    "# Check the count of each intent category\n",
    "intent_counts = {intent: intents['target'].count(intent) for intent in set(intents['target'])}\n",
    "print(\"Intent Counts:\", intent_counts)\n",
    "\n",
    "intents['data'].extend(rephrased_queries)\n",
    "intents['target'].extend(['car models - sedan info' for i in range(len(rephrased_queries))])\n",
    "\n",
    "\n",
    "# Check the count of each intent category\n",
    "intent_counts = {intent: intents['target'].count(intent) for intent in set(intents['target'])}\n",
    "print(\"Intent Counts:\", intent_counts)\n",
    "#intent_counts = {intent: intents['data'].count(intent) for intent in set(intents['data'])}\n",
    "#print(\"Intent Counts:\", intent_counts)\n",
    "import pandas as pd\n",
    "print(\"data:\",len(intents['data']))\n",
    "print(len(intents['target']))\n",
    "#print(len(list(intents[target])))\n",
    "#df=pd.DataFrame(intents)\n",
    "#print(df.value_counts())\n",
    "# Identify underrepresented intents\n",
    "\n",
    "underrepresented_intents = [intent for intent, count in intent_counts.items() if count < 10]\n",
    "\n",
    "# Add more training examples for underrepresented intents\n",
    "additional_training_examples = 100 - min(intent_counts.values())  # Number of examples needed to balance intents\n",
    "\n",
    "for intent in underrepresented_intents:\n",
    "  if intent==\"greetings\":\n",
    "    rephrased_queries_for_intent = [\n",
    "        f\"I want more examples for intent greetings hi hello hey hola howdy {intent}\" for _ in range(additional_training_examples)\n",
    "    ]\n",
    "  else:\n",
    "     rephrased_queries_for_intent = [\n",
    "        f\"I want more examples for intent  {intent}\" for _ in range(additional_training_examples)\n",
    "    ]\n",
    "\n",
    "  intents['data'].extend(rephrased_queries_for_intent)\n",
    "  intents['target'].extend([intent] * additional_training_examples)\n",
    "\n",
    "# Shuffle the data to maintain randomness\n",
    "combined_data = list(zip(intents['data'], intents['target']))\n",
    "random.shuffle(combined_data)\n",
    "intents['data'], intents['target'] = zip(*combined_data)\n",
    "\n",
    "# Check the count of each intent category after balancing\n",
    "intent_counts_after_balance = {intent: intents['target'].count(intent) for intent in set(intents['target'])}\n",
    "print(\"Intent Counts after balancing:\", intent_counts_after_balance)\n",
    "print(\"hilen:\",intents[\"data\"][intents['target']==\"greetings\"])\n",
    "print(len(intents['data']))\n",
    "print(len(intents['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dibbel9qFXzD",
    "outputId": "07a307c2-04c2-4d47-b76c-631d61eb427e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: [ u n u s e d 0 ]\n",
      "User: exit\n",
      "Chatbot: [ u n u s e d 0 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(input_text):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, add_special_tokens=True, truncation=True, max_length=128, padding='max_length')\n",
    "\n",
    "    # Convert tokens to tensor\n",
    "    input_ids_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor)\n",
    "        predicted_label = torch.argmax(outputs.logits)\n",
    "\n",
    "    # Decode response tokens\n",
    "    response_text = tokenizer.decode(predicted_label, skip_special_tokens=True)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Chatbot loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n",
    "    conversation_history.append(response)\n",
    "\n",
    "    # Exit loop if user indicates end of conversation\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "zaepXRrN_Pg9",
    "outputId": "e83c6de3-c312-432d-95b9-7d0787093f19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"auto_df\",\n  \"rows\": 991,\n  \"fields\": [\n    {\n      \"column\": \"data\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 491,\n        \"samples\": [\n          \"I'm interested in knowing about SUV models.\",\n          \"Can you give me details on your SUV lineup?\",\n          \"Can you tell me about the sedan cars you sell?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"car maintenance\",\n          \"car brands\",\n          \"car models\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "auto_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-24830c1e-d42b-4051-a441-95f324dc2d26\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want more examples for intent greetings hi h...</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want more examples for intent  car brands</td>\n",
       "      <td>car brands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me about the truck models at your dealers...</td>\n",
       "      <td>car models - truck info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell me about the coupe models at your dealers...</td>\n",
       "      <td>car models - coupe info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want more examples for intent  car features</td>\n",
       "      <td>car features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>I want more examples for intent greetings hi h...</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>I want more examples for intent  car features</td>\n",
       "      <td>car features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Tell me more about the sedans you have available.</td>\n",
       "      <td>car models - sedan info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Can you give me details on your coupe lineup?</td>\n",
       "      <td>car models - coupe info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>What sedan models do you have available for sale?</td>\n",
       "      <td>car models - sedan info</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24830c1e-d42b-4051-a441-95f324dc2d26')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-24830c1e-d42b-4051-a441-95f324dc2d26 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-24830c1e-d42b-4051-a441-95f324dc2d26');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d8529de7-b0f4-4bb6-bf1e-acd86b608c21\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8529de7-b0f4-4bb6-bf1e-acd86b608c21')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d8529de7-b0f4-4bb6-bf1e-acd86b608c21 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_634e9f89-cb24-4952-9d31-dc38837fec6c\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('auto_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_634e9f89-cb24-4952-9d31-dc38837fec6c button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('auto_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                  data  \\\n",
       "0    I want more examples for intent greetings hi h...   \n",
       "1          I want more examples for intent  car brands   \n",
       "2    Tell me about the truck models at your dealers...   \n",
       "3    Tell me about the coupe models at your dealers...   \n",
       "4        I want more examples for intent  car features   \n",
       "..                                                 ...   \n",
       "986  I want more examples for intent greetings hi h...   \n",
       "987      I want more examples for intent  car features   \n",
       "988  Tell me more about the sedans you have available.   \n",
       "989      Can you give me details on your coupe lineup?   \n",
       "990  What sedan models do you have available for sale?   \n",
       "\n",
       "                      target  \n",
       "0                  greetings  \n",
       "1                 car brands  \n",
       "2    car models - truck info  \n",
       "3    car models - coupe info  \n",
       "4               car features  \n",
       "..                       ...  \n",
       "986                greetings  \n",
       "987             car features  \n",
       "988  car models - sedan info  \n",
       "989  car models - coupe info  \n",
       "990  car models - sedan info  \n",
       "\n",
       "[991 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "auto_df = pd.DataFrame(intents)\n",
    "auto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5YM6fZS_quz",
    "outputId": "a1792efb-b009-4711-e8cf-ac859b9ed510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "greetings                      100\n",
       "car brands                     100\n",
       "car features                   100\n",
       "car models                     100\n",
       "car maintenance                100\n",
       "car models - sedan info         99\n",
       "car models - truck info         98\n",
       "car models - coupe info         98\n",
       "car models - SUV info           98\n",
       "car models - hatchback info     98\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1fGzgfc_4Jp",
    "outputId": "0800f46f-8132-4d89-cb74-115ca65049fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj3Whrr1AAAw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5OAnv3ZG-nM",
    "outputId": "88790763-d99f-4ae4-b886-a9a89c560c5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Validation Loss: 1.5620630383491516\n",
      "Epoch 2/3, Validation Loss: 0.8994157761335373\n",
      "Epoch 3/3, Validation Loss: 0.5019881278276443\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(auto_df['target'])))\n",
    "# Assuming you have a list of unique labels\n",
    "unique_labels = list(set(auto_df['target']))\n",
    "# Create a label map\n",
    "label_map = {label: index for index, label in enumerate(unique_labels)}\n",
    "\n",
    "# Convert labels to a list of numerical values\n",
    "label_list = [label_map[label] for label in auto_df['target']]\n",
    "\n",
    "# Convert the label list to a tensor\n",
    "labels = torch.tensor(label_list)\n",
    "\n",
    "# Convert auto_df.data to a list of strings\n",
    "text_data = auto_df.data.tolist()\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(text_data, labels, test_size=0.1)\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx] for key, tensor in self.inputs.items()}, self.labels[idx]\n",
    "train_dataset = IntentDataset(train_encodings, train_labels)\n",
    "val_dataset = IntentDataset(val_encodings, val_labels)\n",
    "#train_dataset = IntentDataset(train_inputs, train_labels)\n",
    "#val_dataset = IntentDataset(val_inputs, val_labels)\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for batch in val_loader:\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = batch\n",
    "            outputs = model(**inputs)\n",
    "            val_loss = loss_fn(outputs.logits, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "\n",
    "model.save_pretrained(\"fine_tuned_bert_automotive\")\n",
    "import pickle\n",
    "pickle.dump(model,open(\"/content/drive/My Drive/fine_tuned_bert_automotive.pkl\",\"wb\"))\n",
    "#pickle.dump((clf,tfidf_vectorizer),open(\"/content/drive/My Drive/neivebayesclf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjwjXsudM0yw"
   },
   "source": [
    "Importing Libraries:\n",
    "\n",
    "torch, torch.utils.data.Dataset, and torch.utils.data.DataLoader: These are from PyTorch and are used for creating and loading datasets.\n",
    "BertTokenizer and BertForSequenceClassification: These are from the Hugging Face transformers library and are used for tokenizing text data and loading the pre-trained BERT model for sequence classification.\n",
    "train_test_split: This is from scikit-learn and is used to split the dataset into training and validation sets.\n",
    "Defining Tokenizer and Model:\n",
    "\n",
    "The BERT tokenizer is initialized using the BertTokenizer.from_pretrained() method, which loads the tokenizer corresponding to the 'bert-base-uncased' model.\n",
    "The BERT model for sequence classification is initialized using the BertForSequenceClassification.from_pretrained() method, which loads the pre-trained BERT model for sequence classification. The num_labels parameter is set to the number of unique labels in the dataset.\n",
    "Creating Label Map:\n",
    "\n",
    "A label map is created, which maps each unique label to a numerical index. This will be used to convert the labels into numerical values.\n",
    "Data Preprocessing:\n",
    "\n",
    "Labels are converted into numerical values using the label map.\n",
    "Text data from the DataFrame (auto_df.data) is converted into a list of strings.\n",
    "The dataset is split into training and validation sets using train_test_split().\n",
    "Tokenization:\n",
    "\n",
    "The text data is tokenized using the BERT tokenizer. Padding and truncation are applied to ensure uniform length sequences, and the data is returned as PyTorch tensors.\n",
    "Creating PyTorch Datasets:\n",
    "\n",
    "Custom PyTorch datasets (IntentDataset) are created using the tokenized inputs and numerical labels. These datasets will be used to create data loaders for training and validation.\n",
    "Defining Training Parameters:\n",
    "\n",
    "Batch size, number of epochs, and learning rate are defined for training.\n",
    "Creating Data Loaders:\n",
    "\n",
    "Data loaders are created for both training and validation datasets. These data loaders will iterate over batches of data during training and evaluation.\n",
    "Defining Optimizer and Loss Function:\n",
    "\n",
    "AdamW optimizer is used for optimization.\n",
    "Cross-entropy loss function is used for calculating the loss.\n",
    "Training Loop:\n",
    "\n",
    "The training loop runs for the specified number of epochs.\n",
    "Within each epoch, the model is set to training mode, and the training data is iterated over in batches. Gradients are calculated, and model parameters are updated.\n",
    "After each epoch, the model is set to evaluation mode, and the validation data is iterated over to calculate the validation loss.\n",
    "Saving the Model:\n",
    "\n",
    "Finally, the fine-tuned model is saved using the save_pretrained() method.\n",
    "Overall, this code fine-tunes a BERT model for sequence classification on the provided dataset and saves the fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RqJSUeU03I5",
    "outputId": "725394bb-91a0-4313-b8ee-6a354e975f9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Function to generate response\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import random\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "# Function to preprocess user input\n",
    "def preprocess_input(user_input):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(user_input.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english') and word not in string.punctuation]\n",
    "    # Lemmatize\n",
    "    lemmatized_tokens = lemmatize_text(tokens)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Function to calculate semantic similarity between user input and predefined responses\n",
    "def semantic_similarity(user_input, responses):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    response_vectors = tfidf_vectorizer.fit_transform(responses)\n",
    "    user_vector = tfidf_vectorizer.transform([user_input])\n",
    "    similarity_scores = cosine_similarity(user_vector, response_vectors)\n",
    "    return similarity_scores\n",
    "\n",
    "def generate_response(user_input, intent, knowledge_base):\n",
    "    user_input = preprocess_input(user_input)\n",
    "    print(\"user preprocess:\",user_input)\n",
    "\n",
    "    # If the predicted intent is consistently \"car brands\" for new unseen inputs, handle it as a fallback\n",
    "\n",
    "    # Check for specific keywords or patterns in the input to determine if it's unrelated to car brands\n",
    "    unrelated_keywords = [\"weather\", \"news\", \"sports\", \"music\"]\n",
    "    unrelated_keywords1 = [\"suv\",\"coupe\",\"hatchback\",\"truck\",\"sedan\",\"car model\", \"car brands\",\"Toyota\",\"Honda\",\"Ford\",\"Chevrolet\",\"Tesla\",\"car feature\",\"feature\", \"car maintenance\",\"maintenance\",\"brand\",\"model\",\"Toyota\", \"Honda\", \"Ford\", \"Chevrolet\",\"Tesla\"]\n",
    "    #unrelated_keywords2 = [\"Our\", \"maintenance\", \"services\", \"oil change\", \"tire rotation\", \"brake inspection\", \"battery check\", fluid level check]\n",
    "\n",
    "\n",
    "\n",
    "    # Get responses based on intent\n",
    "    responses = knowledge_base.get(intent, [])\n",
    "    print(\"response1:\",responses)\n",
    "     # Check for greetings\n",
    "    greetings1 = [\"hi\", \"hello\", \"hey\", \"hola\", \"howdy\",\"greeting\"]\n",
    "    if intent == \"greetings\" and any(word in user_input for word in greetings1):\n",
    "        return responses\n",
    "\n",
    "    elif intent == \"car maintenance\":\n",
    "      print(user_input)\n",
    "      if user_input  in unrelated_keywords1:\n",
    "        return responses\n",
    "    elif intent=='car models - sedan info' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car models - SUV info' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car models - coupe info' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car models - truck info' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car models - hatchback info' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car features' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car models' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "    elif intent=='car brands' and any(word in user_input for word in unrelated_keywords1):\n",
    "      return responses\n",
    "\n",
    "    else:\n",
    "      return \"It seems like you're asking about something unrelated to automotive industry. Can you please provide more information or ask a different question?\"\n",
    "    # If no responses for the intent, return a default response\n",
    "    if responses==None and intent=='car brands':\n",
    "        return \"Sorry, I don't have information about that. Can you rephrase your question?\"\n",
    "    #return ', '.join(responses)\n",
    "    #return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMTvxvsFKsR3",
    "outputId": "10d89442-5aff-47a7-bd2f-c47c6edbcbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "user preprocess: hi\n",
      "response1: Hello! How can I assist you today?\n",
      "Bot: greetings\n",
      "BOT: Hello! How can I assist you today?\n",
      "You: hello\n",
      "user preprocess: hello\n",
      "response1: Hello! How can I assist you today?\n",
      "Bot: greetings\n",
      "BOT: Hello! How can I assist you today?\n",
      "You: car\n",
      "user preprocess: car\n",
      "response1: Our vehicles come with a range of features including engine options, transmission types, fuel efficiency, and safety features.\n",
      "Bot: car features\n",
      "BOT: It seems like you're asking about something unrelated to automotive industry. Can you please provide more information or ask a different question?\n",
      "You: car model\n",
      "user preprocess: car model\n",
      "response1: Our available models include sedan, SUV, coupe, hatchback, and truck.\n",
      "Bot: car models\n",
      "BOT: Our available models include sedan, SUV, coupe, hatchback, and truck.\n",
      "You: exit\n",
      "Bot: Good By!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_automotive')\n",
    "\n",
    "# Function to classify user input\n",
    "def classify_intent(user_input):\n",
    "    # Tokenize user input\n",
    "    inputs = tokenizer(user_input, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Pass input through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    # Determine the predicted intent index\n",
    "    predicted_intent_index = torch.argmax(probabilities).item()\n",
    "\n",
    "    return predicted_intent_index\n",
    "\n",
    "# Decode intent index to target labels\n",
    "def decode_intent(intent_index):\n",
    "    # Reverse the label map to map index to label\n",
    "    label_map_reverse = {v: k for k, v in label_map.items()}\n",
    "    return label_map_reverse[intent_index]\n",
    "\n",
    "# Example usage\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower()=='exit':\n",
    "      print(\"Bot: Good By!\")\n",
    "      break\n",
    "    intent_index = classify_intent(user_input)\n",
    "    intent_label = decode_intent(intent_index)\n",
    "    response = generate_response(user_input, intent_label, response_templates)\n",
    "    print(\"Bot:\", intent_label)\n",
    "    print(\"BOT:\", response)\n",
    "    # Generate response based on intent label\n",
    "    # Add your response generation logic here based on the predicted intent label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDm91qZcNMpK"
   },
   "source": [
    "This code snippet implements a basic chatbot using the fine-tuned BERT model for sequence classification. Here's a breakdown of what each part of the code does:\n",
    "\n",
    "Load the Fine-Tuned Model:\n",
    "\n",
    "The fine-tuned BERT model for sequence classification is loaded using the BertForSequenceClassification.from_pretrained() method. The model weights are loaded from the directory where the fine-tuned model is saved, which is specified as 'fine_tuned_bert_automotive'.\n",
    "Function to Classify User Input:\n",
    "\n",
    "The classify_intent function takes a user input string, tokenizes it using the BERT tokenizer, and passes it through the loaded model to obtain predictions.\n",
    "The model outputs are passed through a softmax function to obtain probabilities for each class.\n",
    "The index of the class with the highest probability (i.e., the predicted intent) is determined using torch.argmax().\n",
    "Function to Decode Intent Index to Target Labels:\n",
    "\n",
    "The decode_intent function takes an intent index (the output of classify_intent) and maps it back to the corresponding target label using the reverse of the label map created earlier.\n",
    "Example Usage:\n",
    "\n",
    "The script enters an infinite loop where it continuously prompts the user for input using input(\"You: \").\n",
    "If the user inputs \"exit\", the loop breaks, and the chatbot exits with a \"Goodbye\" message.\n",
    "Otherwise, the user input is classified using classify_intent, and the predicted intent label is decoded using decode_intent.\n",
    "The predicted intent label is then printed as the bot's response.\n",
    "This code provides a basic framework for a chatbot that can classify user input into different intents using a fine-tuned BERT model. You can extend it by adding response generation logic based on the predicted intents to make the chatbot more interactive and useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut5zncAYKoMJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7V1WvbqBVyV",
    "outputId": "2c4cbaa4-ae45-4814-fa96-3dc3f71dbba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I want more examples for intent greetings hi h...\n",
       "1            I want more examples for intent  car models\n",
       "2       I want more examples for intent  car maintenance\n",
       "3            I want more examples for intent  car brands\n",
       "4           I want to know more about truck car choices.\n",
       "                             ...                        \n",
       "986          I want more examples for intent  car brands\n",
       "987          I want more examples for intent  car models\n",
       "988    I want more examples for intent greetings hi h...\n",
       "989        What sedan models are part of your inventory?\n",
       "990    I'm interested in knowing about your truck inv...\n",
       "Name: data, Length: 991, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_df['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383,
     "referenced_widgets": [
      "f84ac6ec8b94479c96b00f9e5f435075",
      "afb488899023413db9a5784f4f6c21be",
      "967e4610ee3b4008b36536cd955dfb3b",
      "ef8b9a06b8e5480a897141580bc6bd8a",
      "841f4293a9ab41a1a352c39ac02d294d",
      "6c45108b1c1748eca6bd6f1cf5ec9c7d",
      "3655ce30e761435f9c993f84e3a1caf6",
      "e176b3b4050d4e5daf5e98b9a8250016",
      "f2070736e4374046b74ba49a5a59adc9",
      "353822cabe5645219f39f7e52a062ee5",
      "aa9baac0091744bf99776dac2203cae5",
      "ed746bf1642f4f34b65beca2120290df",
      "d0772f979f184587a062cc1b362c851d",
      "2a42f187656b428ba58b1f72cd9ccb8a",
      "68a4e9f2c0a545d4a19ed93506a8f644",
      "caff926489644114a6b922326c3745b6",
      "3097ecd1f29e45dd9205cfcfbd07a0eb",
      "2fa9d87ce1ee40a4be49d91b53b4e076",
      "e61e8ea005094b9bbf16ae798031a524",
      "f13653b7aca44082b0400b53c59ec75c",
      "cfd5efec8c0f41398dfe85019d183362",
      "7397bb7ec96e4fa797892c8d11ce59ed",
      "2cc256e2b86747bf9142c5bbb84708eb",
      "ad793ab3d62e4e7cb9935e8dbae5abae",
      "94965fdc670e4adfab76efbcf0e2b3da",
      "a6ecabfe6fa54499a93de29efb33ac0b",
      "c55f708a6ad0494496a37c9e36a1f5c5",
      "56f5c5124a4a46d589a536763b5e84a3",
      "79ed0b5280024564a8c90960d20c0621",
      "ac4450101a784a7d9bcab34bf2c0a4eb",
      "12f18951ac58467b8dd42446e650c142",
      "e35895322ce74be9a8328c322189f431",
      "2c173e477e684a33a395f2cef6a71b6b",
      "66d34ea4ab3c49708c1bd77f2a93e32c",
      "9271aa1789634e5ca812cbc44ecac289",
      "16d38801993b4d10950aadb59c40eb50",
      "57ee07481dbf43e7a38aa5f006ea0555",
      "8e122f7a52c54fb783949bbade2602a0",
      "a54d806fc50c46e6815a02a762674bff",
      "40fd43c0e7d543b989863a6c3f6e335c",
      "58da7d2bc66d4aefa5e8cadb2c2698fd",
      "4b86f6c9c9e24e1da6dfc606f5d5e139",
      "a7f1f342180c4ab386893ed7d89f0881",
      "a12e6fb7c6664b60a6f40c403066d132",
      "95ada5be02b24dee81d23f60fdc7f130",
      "b69a9a873a524cd39d3aa5a6735c61c4",
      "8dc3f9d191dd481cb62f437ea6c06e59",
      "6f647cfdb7bc4e10bf8c125a7ee8f4c0",
      "eef399382e204a1e9e3624b941e1080e",
      "cb820438078b46f39c4d74f81b2212a7",
      "753c5d53865040a783c4c334aa03e3c8",
      "9ad01c816f9646008f3d09bb7411de42",
      "253ddf9d0fa240e8997e54cef3c270e9",
      "b7bb8107b1dd47339aa8ba372ff92d69",
      "6d2277e81c7644b199c3feceed119665",
      "b3127bff88ba4952ade8b03f267d9be4",
      "45733621aeee4838a72de414e14a02f2",
      "925217937b8741008bcceb755b59e1d9",
      "d6bdf95e71224ed89d33a964e634f448",
      "da3c55a67160466da59a09721d82da58",
      "b2d1ef4015b645acae2f63945a5d904c",
      "2e4e5c31344543ecaf04a9b509751286",
      "c2e028d3bac545d09f7b11a209ab6c35",
      "d0a22e0ea8b142a695a5bbe303c8588a",
      "47bf2f05a9fb4a3a8652644972760d41",
      "d8f71ffbcf19470d81e464f16cdedd4b",
      "51db72e265ab4bd28917a94737192b21",
      "918690e4a05f4822b75ec991808c2b49",
      "e284a66e3bb442e2a268857e8e1f40a2",
      "46a554cc0e8b4c49bb75fc742a1cbe7b",
      "723e65b60b29492e94d57b293de632e7",
      "d96957fab15147208322145fea47b0cb",
      "d999535df8ed4e8885f8bde922ec34b5",
      "b2b51535ccbe493586c631a9f532e8bc",
      "eb908d659e1342cb9e698c90d76c11ee",
      "e8b8ed78fcc64ca1beb3dd8e14b927bc",
      "783c2bfcbf53450d840ea5b1d4bbe124"
     ]
    },
    "id": "FFwYsLUkFcUH",
    "outputId": "f9360d01-959c-45bc-cf2d-ec9424b46086"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84ac6ec8b94479c96b00f9e5f435075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed746bf1642f4f34b65beca2120290df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc256e2b86747bf9142c5bbb84708eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d34ea4ab3c49708c1bd77f2a93e32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ada5be02b24dee81d23f60fdc7f130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3127bff88ba4952ade8b03f267d9be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51db72e265ab4bd28917a94737192b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: How are you today?\n",
      "\n",
      "I'm fine. I'm fine. I'm fine.\n",
      "\n",
      "What's your favorite thing about being a professional athlete?\n",
      "\n",
      "I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I love the game. I\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"  # You can choose different GPT-2 variants like \"gpt2\", \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define function to generate response\n",
    "def generate_response(prompt, max_length=100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"How are you today?\"\n",
    "response = generate_response(prompt)\n",
    "print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "fhcuF8ZNOotg",
    "outputId": "6e4ef804-9fc6-45db-cc7b-9372e3c3b14a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5c669d179f56>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Create a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m dataset = TextDataset(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# No need for a file path since we provide text directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input file path {file_path} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Convert auto_df data to a list of strings\n",
    "text_data = auto_df.data.tolist()\n",
    "\n",
    "# Tokenize text data\n",
    "tokenized_text = tokenizer(text_data, truncation=True, padding=True)\n",
    "\n",
    "# Convert tokenized text to a flat list of strings\n",
    "flat_tokenized_text = [token for sublist in tokenized_text['input_ids'] for token in sublist]\n",
    "\n",
    "# Define block size (maximum sequence length)\n",
    "block_size = 128  # Adjust this as needed based on your dataset and available memory\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TextDataset(\n",
    "    file_path=None,  # No need for a file path since we provide text directly\n",
    "    tokenizer=tokenizer,\n",
    "    block_size=block_size,\n",
    "\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned-auto\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2-finetuned-auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e4OQZZqN2E8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12f18951ac58467b8dd42446e650c142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16d38801993b4d10950aadb59c40eb50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58da7d2bc66d4aefa5e8cadb2c2698fd",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b86f6c9c9e24e1da6dfc606f5d5e139",
      "value": 1355256
     }
    },
    "253ddf9d0fa240e8997e54cef3c270e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a42f187656b428ba58b1f72cd9ccb8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e61e8ea005094b9bbf16ae798031a524",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f13653b7aca44082b0400b53c59ec75c",
      "value": 1042301
     }
    },
    "2c173e477e684a33a395f2cef6a71b6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2cc256e2b86747bf9142c5bbb84708eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad793ab3d62e4e7cb9935e8dbae5abae",
       "IPY_MODEL_94965fdc670e4adfab76efbcf0e2b3da",
       "IPY_MODEL_a6ecabfe6fa54499a93de29efb33ac0b"
      ],
      "layout": "IPY_MODEL_c55f708a6ad0494496a37c9e36a1f5c5"
     }
    },
    "2e4e5c31344543ecaf04a9b509751286": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fa9d87ce1ee40a4be49d91b53b4e076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3097ecd1f29e45dd9205cfcfbd07a0eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353822cabe5645219f39f7e52a062ee5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3655ce30e761435f9c993f84e3a1caf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40fd43c0e7d543b989863a6c3f6e335c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45733621aeee4838a72de414e14a02f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2d1ef4015b645acae2f63945a5d904c",
      "placeholder": "​",
      "style": "IPY_MODEL_2e4e5c31344543ecaf04a9b509751286",
      "value": "model.safetensors: 100%"
     }
    },
    "46a554cc0e8b4c49bb75fc742a1cbe7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8b8ed78fcc64ca1beb3dd8e14b927bc",
      "placeholder": "​",
      "style": "IPY_MODEL_783c2bfcbf53450d840ea5b1d4bbe124",
      "value": " 124/124 [00:00&lt;00:00, 4.75kB/s]"
     }
    },
    "47bf2f05a9fb4a3a8652644972760d41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b86f6c9c9e24e1da6dfc606f5d5e139": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "51db72e265ab4bd28917a94737192b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_918690e4a05f4822b75ec991808c2b49",
       "IPY_MODEL_e284a66e3bb442e2a268857e8e1f40a2",
       "IPY_MODEL_46a554cc0e8b4c49bb75fc742a1cbe7b"
      ],
      "layout": "IPY_MODEL_723e65b60b29492e94d57b293de632e7"
     }
    },
    "56f5c5124a4a46d589a536763b5e84a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57ee07481dbf43e7a38aa5f006ea0555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7f1f342180c4ab386893ed7d89f0881",
      "placeholder": "​",
      "style": "IPY_MODEL_a12e6fb7c6664b60a6f40c403066d132",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 12.4MB/s]"
     }
    },
    "58da7d2bc66d4aefa5e8cadb2c2698fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66d34ea4ab3c49708c1bd77f2a93e32c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9271aa1789634e5ca812cbc44ecac289",
       "IPY_MODEL_16d38801993b4d10950aadb59c40eb50",
       "IPY_MODEL_57ee07481dbf43e7a38aa5f006ea0555"
      ],
      "layout": "IPY_MODEL_8e122f7a52c54fb783949bbade2602a0"
     }
    },
    "68a4e9f2c0a545d4a19ed93506a8f644": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfd5efec8c0f41398dfe85019d183362",
      "placeholder": "​",
      "style": "IPY_MODEL_7397bb7ec96e4fa797892c8d11ce59ed",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 9.89MB/s]"
     }
    },
    "6c45108b1c1748eca6bd6f1cf5ec9c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2277e81c7644b199c3feceed119665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f647cfdb7bc4e10bf8c125a7ee8f4c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7bb8107b1dd47339aa8ba372ff92d69",
      "placeholder": "​",
      "style": "IPY_MODEL_6d2277e81c7644b199c3feceed119665",
      "value": " 718/718 [00:00&lt;00:00, 34.5kB/s]"
     }
    },
    "723e65b60b29492e94d57b293de632e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7397bb7ec96e4fa797892c8d11ce59ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "753c5d53865040a783c4c334aa03e3c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "783c2bfcbf53450d840ea5b1d4bbe124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79ed0b5280024564a8c90960d20c0621": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "841f4293a9ab41a1a352c39ac02d294d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc3f9d191dd481cb62f437ea6c06e59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ad01c816f9646008f3d09bb7411de42",
      "max": 718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_253ddf9d0fa240e8997e54cef3c270e9",
      "value": 718
     }
    },
    "8e122f7a52c54fb783949bbade2602a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "918690e4a05f4822b75ec991808c2b49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d96957fab15147208322145fea47b0cb",
      "placeholder": "​",
      "style": "IPY_MODEL_d999535df8ed4e8885f8bde922ec34b5",
      "value": "generation_config.json: 100%"
     }
    },
    "925217937b8741008bcceb755b59e1d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2e028d3bac545d09f7b11a209ab6c35",
      "max": 1519984962,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0a22e0ea8b142a695a5bbe303c8588a",
      "value": 1519984962
     }
    },
    "9271aa1789634e5ca812cbc44ecac289": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a54d806fc50c46e6815a02a762674bff",
      "placeholder": "​",
      "style": "IPY_MODEL_40fd43c0e7d543b989863a6c3f6e335c",
      "value": "tokenizer.json: 100%"
     }
    },
    "94965fdc670e4adfab76efbcf0e2b3da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac4450101a784a7d9bcab34bf2c0a4eb",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12f18951ac58467b8dd42446e650c142",
      "value": 456318
     }
    },
    "95ada5be02b24dee81d23f60fdc7f130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b69a9a873a524cd39d3aa5a6735c61c4",
       "IPY_MODEL_8dc3f9d191dd481cb62f437ea6c06e59",
       "IPY_MODEL_6f647cfdb7bc4e10bf8c125a7ee8f4c0"
      ],
      "layout": "IPY_MODEL_eef399382e204a1e9e3624b941e1080e"
     }
    },
    "967e4610ee3b4008b36536cd955dfb3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e176b3b4050d4e5daf5e98b9a8250016",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2070736e4374046b74ba49a5a59adc9",
      "value": 26
     }
    },
    "9ad01c816f9646008f3d09bb7411de42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a12e6fb7c6664b60a6f40c403066d132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a54d806fc50c46e6815a02a762674bff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ecabfe6fa54499a93de29efb33ac0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e35895322ce74be9a8328c322189f431",
      "placeholder": "​",
      "style": "IPY_MODEL_2c173e477e684a33a395f2cef6a71b6b",
      "value": " 456k/456k [00:00&lt;00:00, 18.3MB/s]"
     }
    },
    "a7f1f342180c4ab386893ed7d89f0881": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa9baac0091744bf99776dac2203cae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac4450101a784a7d9bcab34bf2c0a4eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad793ab3d62e4e7cb9935e8dbae5abae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56f5c5124a4a46d589a536763b5e84a3",
      "placeholder": "​",
      "style": "IPY_MODEL_79ed0b5280024564a8c90960d20c0621",
      "value": "merges.txt: 100%"
     }
    },
    "afb488899023413db9a5784f4f6c21be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c45108b1c1748eca6bd6f1cf5ec9c7d",
      "placeholder": "​",
      "style": "IPY_MODEL_3655ce30e761435f9c993f84e3a1caf6",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "b2b51535ccbe493586c631a9f532e8bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2d1ef4015b645acae2f63945a5d904c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3127bff88ba4952ade8b03f267d9be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45733621aeee4838a72de414e14a02f2",
       "IPY_MODEL_925217937b8741008bcceb755b59e1d9",
       "IPY_MODEL_d6bdf95e71224ed89d33a964e634f448"
      ],
      "layout": "IPY_MODEL_da3c55a67160466da59a09721d82da58"
     }
    },
    "b69a9a873a524cd39d3aa5a6735c61c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb820438078b46f39c4d74f81b2212a7",
      "placeholder": "​",
      "style": "IPY_MODEL_753c5d53865040a783c4c334aa03e3c8",
      "value": "config.json: 100%"
     }
    },
    "b7bb8107b1dd47339aa8ba372ff92d69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2e028d3bac545d09f7b11a209ab6c35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c55f708a6ad0494496a37c9e36a1f5c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caff926489644114a6b922326c3745b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb820438078b46f39c4d74f81b2212a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfd5efec8c0f41398dfe85019d183362": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0772f979f184587a062cc1b362c851d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3097ecd1f29e45dd9205cfcfbd07a0eb",
      "placeholder": "​",
      "style": "IPY_MODEL_2fa9d87ce1ee40a4be49d91b53b4e076",
      "value": "vocab.json: 100%"
     }
    },
    "d0a22e0ea8b142a695a5bbe303c8588a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6bdf95e71224ed89d33a964e634f448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47bf2f05a9fb4a3a8652644972760d41",
      "placeholder": "​",
      "style": "IPY_MODEL_d8f71ffbcf19470d81e464f16cdedd4b",
      "value": " 1.52G/1.52G [00:18&lt;00:00, 99.2MB/s]"
     }
    },
    "d8f71ffbcf19470d81e464f16cdedd4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d96957fab15147208322145fea47b0cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d999535df8ed4e8885f8bde922ec34b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da3c55a67160466da59a09721d82da58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e176b3b4050d4e5daf5e98b9a8250016": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e284a66e3bb442e2a268857e8e1f40a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2b51535ccbe493586c631a9f532e8bc",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb908d659e1342cb9e698c90d76c11ee",
      "value": 124
     }
    },
    "e35895322ce74be9a8328c322189f431": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e61e8ea005094b9bbf16ae798031a524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8b8ed78fcc64ca1beb3dd8e14b927bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb908d659e1342cb9e698c90d76c11ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed746bf1642f4f34b65beca2120290df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0772f979f184587a062cc1b362c851d",
       "IPY_MODEL_2a42f187656b428ba58b1f72cd9ccb8a",
       "IPY_MODEL_68a4e9f2c0a545d4a19ed93506a8f644"
      ],
      "layout": "IPY_MODEL_caff926489644114a6b922326c3745b6"
     }
    },
    "eef399382e204a1e9e3624b941e1080e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef8b9a06b8e5480a897141580bc6bd8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_353822cabe5645219f39f7e52a062ee5",
      "placeholder": "​",
      "style": "IPY_MODEL_aa9baac0091744bf99776dac2203cae5",
      "value": " 26.0/26.0 [00:00&lt;00:00, 1.21kB/s]"
     }
    },
    "f13653b7aca44082b0400b53c59ec75c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2070736e4374046b74ba49a5a59adc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f84ac6ec8b94479c96b00f9e5f435075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afb488899023413db9a5784f4f6c21be",
       "IPY_MODEL_967e4610ee3b4008b36536cd955dfb3b",
       "IPY_MODEL_ef8b9a06b8e5480a897141580bc6bd8a"
      ],
      "layout": "IPY_MODEL_841f4293a9ab41a1a352c39ac02d294d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
